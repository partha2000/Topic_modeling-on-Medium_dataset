{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Topic modeling on Medium articles</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/medium-articles-with-content/Medium_AggregatedData.csv\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Tensorflow\t-\t 2.3.0\n",
      "NLTK\t\t-\t 3.2.4\n",
      "Gensim\t\t-\t 3.2.4\n"
     ]
    }
   ],
   "source": [
    "## All minimum setup and libraries required\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "## TENSORFLOW        \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer   ## Generate dictionary of word encodings\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## GENSIM and NLTK\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Tensorflow\\t-\\t\",tf.__version__)\n",
    "print(\"NLTK\\t\\t-\\t\",nltk.__version__)\n",
    "print(\"Gensim\\t\\t-\\t\",nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset have been read\n"
     ]
    }
   ],
   "source": [
    "path = \"../input/medium-articles-with-content/Medium_AggregatedData.csv\"\n",
    "dataframe_full = pd.read_csv(path)\n",
    "dataframe_imp = pd.read_csv(path)\n",
    "print(\"Dataset have been read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audioVersionDurationSec</th>\n",
       "      <th>codeBlock</th>\n",
       "      <th>codeBlockCount</th>\n",
       "      <th>collectionId</th>\n",
       "      <th>createdDate</th>\n",
       "      <th>createdDatetime</th>\n",
       "      <th>firstPublishedDate</th>\n",
       "      <th>firstPublishedDatetime</th>\n",
       "      <th>imageCount</th>\n",
       "      <th>isSubscriptionLocked</th>\n",
       "      <th>...</th>\n",
       "      <th>slug</th>\n",
       "      <th>name</th>\n",
       "      <th>postCount</th>\n",
       "      <th>author</th>\n",
       "      <th>bio</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>usersFollowedByCount</th>\n",
       "      <th>usersFollowedCount</th>\n",
       "      <th>scrappedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>265164.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>5708.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>it</td>\n",
       "      <td>It</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>technology</td>\n",
       "      <td>Technology</td>\n",
       "      <td>166125.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>robotics</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>9103.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   audioVersionDurationSec codeBlock  codeBlockCount  collectionId  \\\n",
       "0                        0       NaN             0.0  638f418c8464   \n",
       "1                        0       NaN             0.0  638f418c8464   \n",
       "2                        0       NaN             0.0  638f418c8464   \n",
       "3                        0       NaN             0.0           NaN   \n",
       "4                        0       NaN             0.0           NaN   \n",
       "\n",
       "  createdDate      createdDatetime firstPublishedDate firstPublishedDatetime  \\\n",
       "0  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "1  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "2  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "3  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "4  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "\n",
       "   imageCount  isSubscriptionLocked  ...        slug        name postCount  \\\n",
       "0           1                 False  ...  blockchain  Blockchain  265164.0   \n",
       "1           1                 False  ...     samsung     Samsung    5708.0   \n",
       "2           1                 False  ...          it          It    3720.0   \n",
       "3          13                 False  ...  technology  Technology  166125.0   \n",
       "4          13                 False  ...    robotics    Robotics    9103.0   \n",
       "\n",
       "         author  bio        userId    userName  usersFollowedByCount  \\\n",
       "0   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "1   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "2   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "3  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "4  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "\n",
       "   usersFollowedCount scrappedDate  \n",
       "0               404.0     20181104  \n",
       "1               404.0     20181104  \n",
       "2               404.0     20181104  \n",
       "3                22.0     20181104  \n",
       "4                22.0     20181104  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Training Mumbai\n",
      "Non-obvious meditation advice from people on the battlefront of daily creation\n",
      "(279577, 50)\n",
      "Big Data Training Mumbai\n",
      "Robotics\n",
      "Meditation\n",
      "Ascent of data Science, SAS and Big data Analyst Trainings Programs\n"
     ]
    }
   ],
   "source": [
    "x = dataframe_full['name'][10]\n",
    "y = dataframe_full['publicationdescription'][15]\n",
    "print(x)\n",
    "print(y)\n",
    "print(dataframe_full.shape)\n",
    "print(dataframe_full['name'][10])\n",
    "print(dataframe_full['name'][11])\n",
    "print(dataframe_full['name'][12])\n",
    "print(dataframe_full['title'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step_1: Preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are ~300000 entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount',\n",
      "       'collectionId', 'createdDate', 'createdDatetime', 'firstPublishedDate',\n",
      "       'firstPublishedDatetime', 'imageCount', 'isSubscriptionLocked',\n",
      "       'language', 'latestPublishedDate', 'latestPublishedDatetime',\n",
      "       'linksCount', 'postId', 'readingTime', 'recommends',\n",
      "       'responsesCreatedCount', 'socialRecommendsCount', 'subTitle',\n",
      "       'tagsCount', 'text', 'title', 'totalClapCount', 'uniqueSlug',\n",
      "       'updatedDate', 'updatedDatetime', 'url', 'vote', 'wordCount',\n",
      "       'publicationdescription', 'publicationdomain',\n",
      "       'publicationfacebookPageName', 'publicationfollowerCount',\n",
      "       'publicationname', 'publicationpublicEmail', 'publicationslug',\n",
      "       'publicationtags', 'publicationtwitterUsername', 'tag_name', 'slug',\n",
      "       'name', 'postCount', 'author', 'bio', 'userId', 'userName',\n",
      "       'usersFollowedByCount', 'usersFollowedCount', 'scrappedDate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_full.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required columns are:\n",
    "* language\n",
    "* subTitle\n",
    "* tagsCount\n",
    "* text\n",
    "* title\n",
    "* url\n",
    "* wordCount\n",
    "* publicationdescription\n",
    "* tag_name\n",
    "* name\n",
    "\n",
    "but the most important columns are primarily:\n",
    "* subTitle\n",
    "* text\n",
    "* title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_col = ['language','subTitle','tagsCount','text','title','url','wordCount','publicationdescription'\n",
    "               ,'tag_name','name']\n",
    "most_imp_col = ['subTitle','text','title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'th' 'ja' 'zh' 'ru' 'pt' 'es' 'zh-Hant' 'id' 'my' 'de' 'tr' 'fr'\n",
      " 'ko' 'it' 'lo' 'un' 'vi' 'cs' 'sk' 'is' 'sv' 'bn' 'mn' 'da' 'no' 'bg'\n",
      " 'ar' 'pl' 'nl' 'ro' 'ca' 'hu' 'hi' 'ka' 'el' 'ms' 'uk' 'si' 'sr' 'lt'\n",
      " 'la' 'fa' 'ml' 'sl' 'mr' 'az' 'lv' 'te' 'mk' 'nn' 'fi']\n"
     ]
    }
   ],
   "source": [
    "# article_titles = dataframe_full['title']\n",
    "# art_grp_1 = article_titles[16:25]\n",
    "# print(art_grp_1)\n",
    "print(dataframe_full.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257655, 50)\n"
     ]
    }
   ],
   "source": [
    "## Number of rows english rows\n",
    "\n",
    "english_titles = dataframe_full[dataframe_full['language'] == 'en']\n",
    "# english_titles.head()\n",
    "print(english_titles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279577, 50)\n",
      "(279577, 50)\n"
     ]
    }
   ],
   "source": [
    "## Number of rows dropped after removing null value rows\n",
    "\n",
    "print(dataframe_imp.shape)\n",
    "dataframe_imp.dropna(how = 'all')\n",
    "print(dataframe_imp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So nothing is missing in any rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257655, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After dropping non-english and columns that are not required really\n",
    "\n",
    "dataframe_imp.drop(dataframe_imp[dataframe_imp['language'] != 'en'].index, inplace = True)\n",
    "\n",
    "dataframe_imp = dataframe_imp.drop(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount',\n",
    "       'collectionId', 'createdDate', 'createdDatetime', 'firstPublishedDate',\n",
    "       'firstPublishedDatetime', 'imageCount', 'isSubscriptionLocked',\n",
    "       'language', 'latestPublishedDate', 'latestPublishedDatetime',\n",
    "       'linksCount', 'postId', 'readingTime', 'recommends',\n",
    "       'responsesCreatedCount', 'socialRecommendsCount','tagsCount','totalClapCount', 'uniqueSlug',\n",
    "       'updatedDate', 'updatedDatetime', 'url', 'vote', 'wordCount',\n",
    "       'publicationdescription', 'publicationdomain',\n",
    "       'publicationfacebookPageName', 'publicationfollowerCount',\n",
    "       'publicationname', 'publicationpublicEmail', 'publicationslug',\n",
    "       'publicationtags', 'publicationtwitterUsername', 'tag_name', 'slug',\n",
    "       'name', 'postCount', 'author', 'bio', 'userId', 'userName',\n",
    "       'usersFollowedByCount', 'usersFollowedCount', 'scrappedDate'], axis=1)\n",
    "\n",
    "dataframe_imp['index'] = dataframe_imp.index\n",
    "\n",
    "dataframe_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subTitle</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A major private IT company implements blockcha...</td>\n",
       "      <td>Private Business, Government and Blockchain\\n\\...</td>\n",
       "      <td>Private Business, Government and Blockchain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A major private IT company implements blockcha...</td>\n",
       "      <td>Private Business, Government and Blockchain\\n\\...</td>\n",
       "      <td>Private Business, Government and Blockchain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A major private IT company implements blockcha...</td>\n",
       "      <td>Private Business, Government and Blockchain\\n\\...</td>\n",
       "      <td>Private Business, Government and Blockchain</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Introduction</td>\n",
       "      <td>EPQ draft 1 (4844 words)\\nhttps://upload.wikim...</td>\n",
       "      <td>EPQ draft 1 (4844 words)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Introduction</td>\n",
       "      <td>EPQ draft 1 (4844 words)\\nhttps://upload.wikim...</td>\n",
       "      <td>EPQ draft 1 (4844 words)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            subTitle  \\\n",
       "0  A major private IT company implements blockcha...   \n",
       "1  A major private IT company implements blockcha...   \n",
       "2  A major private IT company implements blockcha...   \n",
       "3                                       Introduction   \n",
       "4                                       Introduction   \n",
       "\n",
       "                                                text  \\\n",
       "0  Private Business, Government and Blockchain\\n\\...   \n",
       "1  Private Business, Government and Blockchain\\n\\...   \n",
       "2  Private Business, Government and Blockchain\\n\\...   \n",
       "3  EPQ draft 1 (4844 words)\\nhttps://upload.wikim...   \n",
       "4  EPQ draft 1 (4844 words)\\nhttps://upload.wikim...   \n",
       "\n",
       "                                         title  index  \n",
       "0  Private Business, Government and Blockchain      0  \n",
       "1  Private Business, Government and Blockchain      1  \n",
       "2  Private Business, Government and Blockchain      2  \n",
       "3                     EPQ draft 1 (4844 words)      3  \n",
       "4                     EPQ draft 1 (4844 words)      4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can a robot love us better than another human can?\n",
      "I discussed this with Michelle Tsng on my Podcast “Crazy Wisdom”.\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_imp.title[15])\n",
    "print(dataframe_imp.subTitle[15])\n",
    "# print(dataframe_imp.text[15])      ## Text is too huge to be displayed\n",
    "print(dataframe_imp.index[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**after dropping all the non english rows and after dropping all non essential columns `dataframe_imp` is the required dataframe****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                          257655\n",
       "unique                          64417\n",
       "top       10 new things to read in AI\n",
       "freq                              186\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list = dataframe_imp['title'].astype(str)   ## using astype(str) eliminates the floting type error\n",
    "title_list.describe()\n",
    "# title_list.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization and stem preprocessing steps on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemmer initialization\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for lemmatization, removal of Stopwords\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "Machine Learning Made Easy: What it is and How it Works\n",
      "['Machine', 'Learning', 'Made', 'Easy:', 'What', 'it', 'is', 'and', 'How', 'it', 'Works']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['machin', 'learn', 'easi', 'work']\n"
     ]
    }
   ],
   "source": [
    "### Code to check the function\n",
    "\n",
    "doc_sample = dataframe_imp[dataframe_imp['index'] == 1000].values[0][2]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "    \n",
    "print(dataframe_imp[dataframe_imp['index'] == 1000].values[0][2])\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                          257655\n",
       "unique                          64417\n",
       "top       10 new things to read in AI\n",
       "freq                              186\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list = dataframe_imp['title'].astype(str)   ## using astype(str) eliminates the floting type error\n",
    "title_list.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34    [meta, model, meta, meta, model, deep, learn]\n",
       "35    [meta, model, meta, meta, model, deep, learn]\n",
       "36               [tip, data, scienc, team, succeed]\n",
       "37               [tip, data, scienc, team, succeed]\n",
       "38               [tip, data, scienc, team, succeed]\n",
       "39                                   [trust, trust]\n",
       "40                                   [trust, trust]\n",
       "41                                   [trust, trust]\n",
       "42                                   [trust, trust]\n",
       "43                                   [trust, trust]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The titles are preprocessed and saved into processd_docs\n",
    "\n",
    "processed_titles = title_list.map(preprocess)\n",
    "processed_titles[30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step_2: Creation of the Bag of words\n",
    "Bag of words is a frequency count of the words occuring in the `preprocessed_docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blockchain\n",
      "1 busi\n",
      "2 govern\n",
      "3 privat\n",
      "4 draft\n",
      "5 word\n",
      "6 analyst\n",
      "7 ascent\n",
      "8 data\n",
      "9 program\n",
      "10 scienc\n"
     ]
    }
   ],
   "source": [
    "## bow --> Bag of Words\n",
    "\n",
    "bow = gensim.corpora.Dictionary(processed_titles)\n",
    "\n",
    "## Finding out words with a min_occurance = 10\n",
    "\n",
    "min_occurance = 10\n",
    "count = 0\n",
    "for k, v in bow.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > min_occurance:    # We can limit the selection based on the frequency\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tokens based on\n",
    "* less than 15 occurances in titles\n",
    "* more than 0.5 of total titles\n",
    "* after the above two steps, keep only the first 100000 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5031 unique tokens: ['blockchain', 'busi', 'govern', 'privat', 'draft']...)\n"
     ]
    }
   ],
   "source": [
    "bow.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the doc2bow dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each title we create a dictionary reporting how many words and how many times those words appear. This is saved to the `bow_corpus`.\n",
    "\n",
    "##### **NOTE:** This step gives a simmillar result for a very small corpus such as title of the articles, but it is important while working on the actual body of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1)],\n",
       " [(4, 1), (5, 1)],\n",
       " [(4, 1), (5, 1)],\n",
       " [(4, 1), (5, 1)],\n",
       " [(6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1)],\n",
       " [(6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1)],\n",
       " [(6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [bow.doc2bow(doc) for doc in processed_titles]\n",
    "bow_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of the BOW for the preprocessed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 126 (\"fake\") appears 1 time.\n",
      "Word 294 (\"photo\") appears 1 time.\n",
      "Word 610 (\"believ\") appears 1 time.\n",
      "Word 611 (\"game\") appears 1 time.\n",
      "Word 612 (\"generat\") appears 1 time.\n",
      "Word 613 (\"mous\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "## A example of the BOW for the 1000th title\n",
    "\n",
    "bow_example = bow_corpus[1000]\n",
    "for i in range(len(bow_example)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_example[i][0], \n",
    "           bow[bow_example[i][0]], \n",
    "           bow_example[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step_3: TF-IDF \n",
    "TF-IDF stands for term frequency–inverse document frequency. The higher the TF-IDF score the rarer a word is in a given corpus and vice-versa. We will be using the TF-IDF model for the gensim models library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.37920465251741053), (1, 0.3610050132437884), (2, 0.5619103680400833), (3, 0.640418574223968)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "for i in corpus_tfidf:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step_4: Running LDA algo on the bag of words\n",
    "Testing LDA(Latent Dirichlet allocation) on the BOW. We will be training our LDA model using `gensim.models.LdaMulticore` and save it to `lda_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA when the num_topics = 10\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=bow, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each topic, we will explore the words occuring in that topic and its relative weight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0 \n",
      "Words: 0.260*\"learn\" + 0.155*\"machin\" + 0.080*\"deep\" + 0.018*\"regress\" + 0.015*\"guid\" + 0.013*\"introduct\" + 0.012*\"model\" + 0.012*\"linear\" + 0.011*\"python\" + 0.009*\"beginn\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.028*\"start\" + 0.028*\"drive\" + 0.027*\"tensorflow\" + 0.026*\"model\" + 0.022*\"self\" + 0.021*\"problem\" + 0.020*\"blockchain\" + 0.017*\"get\" + 0.017*\"meet\" + 0.015*\"team\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.039*\"robot\" + 0.035*\"custom\" + 0.031*\"human\" + 0.027*\"digit\" + 0.024*\"experi\" + 0.017*\"think\" + 0.016*\"year\" + 0.015*\"interact\" + 0.014*\"bot\" + 0.014*\"case\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.057*\"futur\" + 0.036*\"design\" + 0.033*\"work\" + 0.027*\"thing\" + 0.024*\"real\" + 0.024*\"time\" + 0.023*\"convers\" + 0.022*\"read\" + 0.021*\"better\" + 0.020*\"need\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.258*\"data\" + 0.088*\"scienc\" + 0.026*\"scientist\" + 0.026*\"python\" + 0.025*\"visual\" + 0.025*\"analyt\" + 0.015*\"analysi\" + 0.014*\"person\" + 0.012*\"social\" + 0.010*\"explor\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.086*\"chatbot\" + 0.026*\"autom\" + 0.026*\"develop\" + 0.026*\"predict\" + 0.025*\"detect\" + 0.020*\"build\" + 0.016*\"text\" + 0.016*\"make\" + 0.014*\"simpl\" + 0.013*\"base\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.142*\"intellig\" + 0.121*\"artifici\" + 0.041*\"technolog\" + 0.027*\"industri\" + 0.023*\"busi\" + 0.017*\"market\" + 0.016*\"futur\" + 0.015*\"chang\" + 0.015*\"healthcar\" + 0.014*\"talk\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.082*\"network\" + 0.065*\"neural\" + 0.021*\"startup\" + 0.016*\"convolut\" + 0.015*\"vision\" + 0.015*\"paper\" + 0.014*\"like\" + 0.013*\"seri\" + 0.013*\"world\" + 0.013*\"newslett\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.055*\"week\" + 0.040*\"imag\" + 0.039*\"algorithm\" + 0.032*\"code\" + 0.027*\"generat\" + 0.020*\"issu\" + 0.020*\"reinforc\" + 0.018*\"search\" + 0.017*\"classif\" + 0.016*\"updat\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.047*\"googl\" + 0.034*\"tech\" + 0.025*\"news\" + 0.022*\"languag\" + 0.021*\"open\" + 0.021*\"product\" + 0.021*\"manag\" + 0.020*\"process\" + 0.019*\"cloud\" + 0.018*\"test\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('\\nTopic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">TODO: Visualization needs to be done to rank the topics based on the weights of words.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
